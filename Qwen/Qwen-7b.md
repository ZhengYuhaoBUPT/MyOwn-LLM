# Qwen-7B æœ¬åœ°éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£å°†æŒ‡å¯¼ä½ å¦‚ä½•åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šéƒ¨ç½² [Qwen-7B](https://github.com/QwenLM/Qwen-7B) æ¨¡å‹ï¼Œå¹¶è¿è¡Œä¸€ä¸ªåŸºç¡€çš„ Web API æœåŠ¡ã€‚

## ğŸ“¦ ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£… Git Large File Storageï¼ˆç”¨äºæ‹‰å–å¤§æ¨¡å‹æƒé‡ï¼‰
# å¦‚æœä½ è¿˜æœªå®‰è£… git-lfsï¼Œè¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š
sudo apt update
sudo apt install git-lfs -y

# åˆå§‹åŒ– git-lfs
git lfs install
```

## ğŸ§± å¯åŠ¨ tmux ä¼šè¯ï¼ˆå¯é€‰ï¼‰

```bash
# æ¨èåœ¨ tmux ä¸‹è¿è¡Œï¼Œé¿å…ä¸­æ–­
tmux
```

## ğŸ”„ å…‹éš†æ¨¡å‹ä»£ç å’Œæƒé‡

```bash
# å…‹éš† Qwen-7B æ¨¡å‹ä»£ç 
git clone https://github.com/QwenLM/Qwen-7B.git

# ä» ModelScope å…‹éš† Qwen-7B-Chat æƒé‡ï¼ˆåŒ…å« tokenizer ç­‰é…ç½®ï¼‰
git clone https://www.modelscope.cn/qwen/Qwen-7B-Chat.git qwen-7b-chat
```

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# å®‰è£… Qwen-7B æ¨¡å‹æ‰€éœ€çš„ä¾èµ–
pip install -r ~/Qwen-7B/requirements.txt

# å®‰è£… Qwen-7B-Chat çš„é¢å¤–ä¾èµ–
pip install -r ~/qwen-7b-chat/requirements.txt
```

âš ï¸ å®‰è£…è¾ƒæ…¢æ—¶å»ºè®®ä½¿ç”¨å›½å†…é•œåƒï¼š

```bash
# ä½¿ç”¨é˜¿é‡Œäº‘é•œåƒåŠ é€Ÿ
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

## ğŸš€ å¯åŠ¨ Web æœåŠ¡

```bash
# å¯åŠ¨ web_demo.pyï¼ŒæŒ‡å®šæ¨¡å‹æƒé‡è·¯å¾„å’Œç›‘å¬åœ°å€
cd ~/Qwen-7B
python web_demo.py -c ../qwen-7b-chat --server-name 0.0.0.0 --server-port 7860
```

å¯åŠ¨æˆåŠŸåï¼ŒWeb API ä¼šè¿è¡Œåœ¨ï¼š

```
http://<ä½ çš„æœåŠ¡å™¨IP>:7860
```

---

## âœ… å¸¸è§é—®é¢˜

- **git lfs ä¸æ˜¯å‘½ä»¤ï¼Ÿ**
  > è¯·ç¡®ä¿æ‰§è¡Œäº† `sudo apt install git-lfs`ï¼Œå¹¶å†æ¬¡è¿è¡Œ `git lfs install`ã€‚

- **ä¾èµ–å®‰è£…å¤ªæ…¢ï¼Ÿ**
  > ä½¿ç”¨å›½å†…é•œåƒæºï¼Œä¾‹å¦‚æ¸…åæˆ–é˜¿é‡Œäº‘é•œåƒï¼Œå¯å¤§å¹…æå‡é€Ÿåº¦ã€‚

---

## ğŸ§  æ³¨æ„äº‹é¡¹

- é»˜è®¤æ¨¡å‹ä¸º Qwen-7Bï¼Œéƒ¨ç½²æ—¶éœ€é¢„ç•™çº¦ 16GB GPU æ˜¾å­˜ã€‚
- è‹¥ä½¿ç”¨ CPU æˆ–ä½æ˜¾å­˜ GPUï¼Œè¯·ä½¿ç”¨ `--cpu-only` å‚æ•°æˆ–å°è¯•é‡åŒ–æ¨¡å‹ã€‚

---

## ğŸ¤ª æ‹‰å–Qwençš„Pythonè¿è¡Œç¨‹åº

```bash
wget https://raw.githubusercontent.com/ZhengYuhaoBUPT/MyOwn-LLM/main/Qwen/StartQwen.py
```
- ä¹Ÿå¯ä»¥ä½¿ç”¨batchæ¨ç†åŠ é€Ÿ
```bash
wget https://raw.githubusercontent.com/ZhengYuhaoBUPT/MyOwn-LLM/main/Qwen/Batch_Inference.py
```

---

## é‡åŒ–

- é¦–å…ˆå®‰è£…é‡åŒ–åŒ…
pip install auto-gptq optimum
- ä»modelscopeä¸‹è½½Qwençš„Int4é‡åŒ–æ¨¡å‹
git clone https://www.modelscope.cn/Qwen/Qwen-7B-Chat-Int4.git

- ä¿®æ”¹ä¹‹å‰çš„å¯åŠ¨ä»£ç 
```bash
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

- å¯ä»¥ç›´æ¥ç”¨åœ¨githubä¸­ä¿®æ”¹çš„ç‰ˆæœ¬
```bash
wget https://raw.githubusercontent.com/ZhengYuhaoBUPT/MyOwn-LLM/main/Qwen/GPTQ_Batch_Inference.py
```

- æ³¨æ„ç‰ˆæœ¬è¦æ±‚
> torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1
torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0
---


## ğŸ“š å‚è€ƒ

- [Qwen-7B GitHub](https://github.com/QwenLM/Qwen-7B)
- [Qwen-7B ModelScope æƒé‡](https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary)
